{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from Metrics import *\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_set, valid_set, dicts = pickle.load(open('atis.pkl','rb'), encoding='latin1')\n",
    "\n",
    "# Create index to word/label dicts\n",
    "w2idx, ne2idx, labels2idx = dicts['words2idx'], dicts['tables2idx'], dicts['labels2idx']\n",
    "idx2w  = {w2idx[k]:k for k in w2idx}\n",
    "idx2ne = {ne2idx[k]:k for k in ne2idx}\n",
    "idx2la = {labels2idx[k]:k for k in labels2idx}\n",
    "idx2w[len(idx2w)]='X' # for padding\n",
    "idx2la[len(idx2la)]='X' # for padding\n",
    "\n",
    "# Create Training/Val data\n",
    "train_x, train_ne, train_label = train_set\n",
    "val_x, val_ne, val_label = valid_set\n",
    "\n",
    "# Tranfer coding to words for check\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in val_x]\n",
    "groundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in val_label]\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in train_x]\n",
    "groundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in train_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check training data\n",
    "from IPython.core.display import display, HTML\n",
    "import tabulate\n",
    "print(\"Training sentences: {}\".format(len(words_train)))\n",
    "for i in range(12):\n",
    "    i=np.random.randint(0,len(words_train))\n",
    "    print(\"Sample no.{}\".format(i))\n",
    "    table= [words_train[i],groundtruth_train[i]]\n",
    "    display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(idx2la)\n",
    "n_vocab = len(idx2w)\n",
    "# n_vocab-1 => latest index number\n",
    "\n",
    "# find max length, need consider both traning and val data\n",
    "maxlen_train_x=np.max([len(train_x[i]) for i in range(len(train_x))])\n",
    "maxlen_val_x=np.max([len(val_x[i]) for i in range(len(val_x))])\n",
    "maxlen=max(maxlen_train_x,maxlen_val_x)\n",
    "\n",
    "#padding data to make all sentences have same length (maxlen)\n",
    "train_x=pad_sequences(train_x,value=n_vocab-1,maxlen=maxlen) # pad value= n_vocab-1\n",
    "train_label=pad_sequences(train_label,value=n_classes-1,maxlen=maxlen) # pad value= n_classes-1= 127\n",
    "val_x=pad_sequences(val_x,value=n_vocab-1,maxlen=maxlen) \n",
    "val_label=pad_sequences(val_label,value=n_classes-1,maxlen=maxlen)\n",
    "\n",
    "#Encode label data to one-hot format\n",
    "train_label=np.eye(n_classes)[train_label] #(4978, 46, 128)\n",
    "val_label=np.eye(n_classes)[val_label] #(893, 46, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model conv1d, GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(64,5,padding='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy',metrics=['accuracy',precision,recall,fbeta_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model Bidirectional LSTM API\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "main_input = Input(shape=(train_x.shape[1],))\n",
    "x = Embedding(n_vocab,100)(main_input)\n",
    "x= Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "x= Dropout(0.25)(x)\n",
    "lstm_out=TimeDistributed(Dense(n_classes, activation='softmax'))(x)\n",
    "model = Model(main_input,lstm_out)\n",
    "#from keras.utils import multi_gpu_model\n",
    "#try:\n",
    "#    model = multi_gpu_model(model)\n",
    "#except:\n",
    "#    pass\n",
    "model.compile('rmsprop', 'categorical_crossentropy',metrics=['accuracy',precision,recall,fbeta_score])\n",
    "model.summary()\n",
    "#Print model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "a = datetime.now().replace(microsecond=0)\n",
    "BATCH_SIZE = 2000\n",
    "model.fit(train_x, train_label,batch_size=BATCH_SIZE,epochs=140,validation_split=0.1)\n",
    "b = datetime.now().replace(microsecond=0)\n",
    "print(b-a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "BATCH_SIZE = 2048\n",
    "score = model.evaluate(val_x, val_label,batch_size=BATCH_SIZE, verbose=1)\n",
    "print(\"Lose:%1.4f \\naccuracy:%1.4f \\nprecision:%1.4f \\nrecall:%1.4f \\nfbeta_score:%1.4f\"%(\n",
    "    score[0],score[1],score[2],score[3],score[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Save trained witghts\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# save model\n",
    "json_string = model.to_json()\n",
    "open('model_architecture.json','w').write(json_string)\n",
    "\n",
    "# read model\n",
    "#json_string=open('model_architecture.json','r').readlines() \n",
    "#model = model_from_json(json_string[0])\n",
    "\n",
    "# save weights\n",
    "#model.save_weights('model_weights_API_epochs140.h5',overwrite=True)\n",
    "\n",
    "# load weights\n",
    "#model.load_weights('model_weights_API_epochs140.h5') #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance, need remove \n",
    "False_count=0\n",
    "False_sum=0\n",
    "Total_sum=0\n",
    "# val_x.shape=(sample,maxlen)\n",
    "pred = model.predict(val_x) #pred.shape=(sample, maxlen, n_classes)\n",
    "pred_arr = np.argmax(pred,axis=-1) #pred_arr.shape=(sample,maxlen), find last dim max value's index number\n",
    "\n",
    "# val_label.shape=(sample,maxlen,n_classes)\n",
    "# val_label[i].shape=(maxlen,n_classes), i sample's label\n",
    "for i in range(len(val_x)):\n",
    "    truth=np.argmax(val_label[i],axis=-1) # val_label[i].shape=(maxlen,n_classes), find last dim max value's index number\n",
    "    Total_sum+=np.sum(truth!=n_classes-1) # Count all words trhough all samples, exclude padding \n",
    "\n",
    "for i in range(len(val_x)):\n",
    "    pred=pred_arr[i]\n",
    "    truth=np.argmax(val_label[i],axis=-1)\n",
    "    if np.all(pred == truth)==False: # If any sentences with any predict wrong word\n",
    "        False_count+=1 # Count by sentences\n",
    "        False_sum+=np.sum(pred != truth) # Count by words\n",
    "print(\"No.%4d \\nError:%4d/%4d \\nAccuracy by word:%3.2f%% \\nAccuracy by sentences:%3.2f%%\" %(\n",
    "    False_count,False_sum,Total_sum,\n",
    "    100-False_sum/Total_sum*100,\n",
    "    100-False_count/len(val_x)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance visual check\n",
    "from IPython.core.display import display, HTML\n",
    "import tabulate\n",
    "False_count=0\n",
    "False_sum=0\n",
    "Total_sum=0\n",
    "\n",
    "#val_x.shape=(sample,maxlen)\n",
    "pred = model.predict(val_x) #pred.shape=(sample, maxlen, n_classes)\n",
    "pred_arr = np.argmax(pred,axis=-1) #pred_arr.shape=(sample,maxlen), find last dim max value's index number\n",
    "\n",
    "for i in range(len(val_x)):\n",
    "    truth=np.argmax(val_label[i],axis=-1).reshape(-1,)\n",
    "    Total_sum+=np.sum(truth!=n_classes-1)\n",
    "\n",
    "for i in range(len(val_x)):\n",
    "    pred=pred_arr[i]\n",
    "    truth=np.argmax(val_label[i],axis=-1).reshape(-1,)\n",
    "    if np.all(pred == truth)==False:\n",
    "        False_count+=1\n",
    "        False_sum+=np.sum(pred != truth)\n",
    "        val_input=[idx2w[x] for x in val_x[i] if x!=n_vocab-1]\n",
    "        pred_label = [idx2la[x] for x in pred if x!=n_classes-1 ]\n",
    "        truth_label = [idx2la[x] for x in truth if x!=n_classes-1]\n",
    "        truth_ind=((pred == truth)[len(pred == truth)-len(val_input):])\n",
    "        table= [val_input,pred_label,truth_label,truth_ind]\n",
    "        display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
